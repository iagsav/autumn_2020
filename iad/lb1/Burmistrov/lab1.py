# -*- coding: utf-8 -*-
"""Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Go9L7xProHSvXTlfCTtKIs0B3ku-tk7
"""

!pip install -U keras-tuner

import numpy as np
import pandas as pd
from keras.datasets import boston_housing 
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization
from kerastuner.engine import hyperparameters
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import time
import math
from keras import utils

import IPython
import matplotlib.pyplot as plt
from keras.layers import Dropout
from sklearn.preprocessing import MinMaxScaler

"""Чтение и вывод исходных данных"""

x = pd.read_csv("/content/drive/My Drive/train.csv")

x

"""Сохранение верных ответов"""

y = x.get("revenue")
y

"""Удаление лишних столбцов"""

x.drop('revenue', axis='columns', inplace=True)
x.drop('Id', axis='columns', inplace=True)
x.head()

"""Получение списка признаков, зашифрованных буквенно"""

cat_vars = [var for var in x.columns if x[var].dtypes == 'O']
print('Number of categorical variables: ', len(cat_vars))

x[cat_vars]

"""Категориальные признаки шифруются через Label Encoder в числа"""

df = pd.DataFrame(x[cat_vars])
x[cat_vars] = df.apply(preprocessing.LabelEncoder().fit_transform)

x[cat_vars]

"""Проверка на наличие NaN"""

def check_nans(data):
  nan_cols = []
  for col_name, col_info in data.items():
    for line_number, line_info in col_info.items():
      if math.isnan(line_info):
        print(f"{col_name } is NaNs")
        nan_cols.append(col_name)
        break
  if nan_cols == []:
    print("[INFO] THERE IS NO NaNS IN DATA!")
  return nan_cols

nan_cols = check_nans(x)

"""Нормализация данных"""

mean = x.mean(axis=0)
std = x.std(axis=0)
x -= mean
x /= std
x.head()

"""### Создание и обучение нейросети

Деление данных на тренировочные и проверочные
"""

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2, test_size = 0.2)

"""Создание и обучение нейросети"""

model = Sequential()

model.add(Dense(512, activation='relu', input_shape=(x_train.shape[1],)))
model.add(Dropout(0.1))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
#model.add(Dense(32, activation='relu'))
model.add(Dense(1))


model.compile(loss='mse', optimizer='adam', metrics=['mae'])
model.summary()
history = model.fit(x_train, y_train, epochs=50, batch_size=1, verbose=1, validation_split=0.3)
history = history.history

"""Построение графика"""

def draw_graph(history):

    loss_values = history["loss"]
    validation_loss_values = history["val_loss"]
    epochs = range(1, len(history['loss']) + 1)
    plt.plot(epochs, loss_values, 'b', label='Training loss')
    plt.plot(epochs, validation_loss_values, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Value')
    plt.legend()
    plt.show()
    
    plt.clf()
 
    mae = history['mae']
    val_mae = history['val_mae']
    plt.plot(epochs, mae, 'r', label='Training mae')
    plt.plot(epochs, val_mae, 'b', label='Validation mae')
    plt.title('Training and validation mae')
    plt.xlabel('Epochs')
    plt.ylabel('mae')
    plt.legend()
    plt.show()

draw_graph(history)

pred = model.predict(x_test)
predicted_x = np.reshape(pred, (pred.shape[0]))
predicted_y = np.reshape(y_test, (y_test.shape[0]))

for test_index in range(20):
  print("Predict:", predicted_x[test_index],", True:",predicted_y.values[test_index])

"""Предсказания и подсчет коэффициента корреляции"""

cc = np.corrcoef(predicted_x, predicted_y)
ccc = cc[0][1]
print(f'Correlation Coefficient: {ccc}')

"""**Keras Tuner**"""

def build_model(hp):
  hidden_layers = hp.Choice('hidden_layers', values=[1,2,3])
  activation_choice = hp.Choice('activation', values=['relu', 'selu', 'elu'])
  model = Sequential()
  model.add(Dense(units=hp.Int('units',min_value=128,max_value=512,step=32),activation=activation_choice, input_shape=(x_train.shape[1], )))
  model.add(Dropout(0.3))
  for i in range(hidden_layers):
    model.add(Dense(units=hp.Int(f'layer_{i}_units_',min_value=64//(i+1), max_value=256//(i+1),step=16//(i+1)),activation=activation_choice))
  model.add(Dense(1))  
  model.compile(optimizer='rmsprop', loss="mse", metrics=["mae"])
  return model

def find_best(x_train, y_train):
  # создаю тюнер, который сможет подобрать оптимальную архитектуру модели
  tuner = RandomSearch(build_model, objective="loss", max_trials=30, executions_per_trial=1)
  print("\n\n\n")
  # начинается автоматический подбор гиперпараметров
  print('[INFO] start searching')
  tuner.search(x_train, y_train, batch_size=1, epochs=50, validation_split=0.3)
  # выбираем лучшую модель
  print("\n\n\nRESULTS SUMMARY")
  tuner.results_summary()
  print("\n\n\n")
  # получаем лучшую модель
  print("\n\n\nHERE IS THE BEST MODEL\n\n\n")
  best_params = tuner.get_best_hyperparameters()[0]
  best_model = tuner.hypermodel.build(best_params)
  best_model.summary()
  return best_model
  

best_model = find_best(x_train, y_train)

best_history = best_model.fit(x_train, y_train, epochs=50, batch_size=1, validation_split=0.3)
best_history = best_history.history
print("[INFO] Training has been finished")

draw_graph(best_history)

kpredicted_x = best_model.predict(x_test)
kpredicted_x = np.reshape(kpredicted_x, (kpredicted_x.shape[0]))

for test_index in range(20):
  print("K:", kpredicted_x[test_index], " M:", predicted_x[test_index], " True:",predicted_y.values[test_index])

KK = np.corrcoef(kpredicted_x, y_test)
KK = KK[0][1]
print(f'My model Correlation Coefficient: {ccc}')
print(f'Keras Correlation Coefficient: {KK}')
# -*- coding: utf-8 -*-
"""lab_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LTcBgv0P3S0oXs8v-NKNgucLygBmrUIl

Чумарина Екатерина

18 АС

датасет mnist, сеть xception
"""

import sys
import os
import cv2
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
import random
import keras
from keras.datasets import mnist
import numpy as np
from keras.utils import to_categorical
import tensorflow as tf
from keras.layers import *
from keras.optimizers import *
from keras.applications import *
from keras.models import Model
from keras.models import load_model
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from keras import backend as k

from google.colab import drive
drive.mount('/content/drive')

nb_classes = 10  # количество классов в датасете(цифры от 0 до 9)
basic_img_width, basic_img_height = 28, 28  # размер изображения
batch_size = 256 
transformation_ratio = .05

"""обработка данных"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# уменьшаем размер датасета вдвое
x_train = x_train[:30_000]
y_train = y_train[:30_000]
x_test = x_test[:5000]
y_test = y_test[:5000]

# меняем форму на ту, которая указана в слое
x_train = x_train.reshape((30_000, basic_img_width, basic_img_height, 1))
# преобразуем к типа float и нормализуем
x_train = x_train.astype('float32') / 255

x_test = x_test.reshape((5000, basic_img_width, basic_img_height, 1))
x_test = x_test.astype('float32') / 255

# one-hot encoding
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

print(f'x_train shape is {x_train.shape}')
print(f'y_train shape is {y_train.shape}')
print(f'x_test shape is {x_test.shape}')
print(f'y_test shape is {y_test.shape}')

"""Вывод примера изображений"""

#пример картинок в датасете
num_row = 2
num_col = 5
num_pics = 10

fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))
for i in range(num_pics):
    ax = axes[i//num_col, i%num_col]
    ax.imshow(x_train[i][:, :, 0], cmap='gray')
plt.tight_layout()
plt.show()

"""Создание копии датасета с другим размером фото"""

# минимальный размер фото для сети XCeption
img_width, img_height = 71, 71

# функиця меняет размер фото так, чтобы сеть XCeption могла его обработать
def resize_dataSet(img):
  img = img[:,:,0]
  res_img = cv2.resize(img, (img_width,img_height))
  res_img = cv2.cvtColor(res_img,cv2.COLOR_GRAY2RGB)
  return res_img

# Создается новый датасет такого же размера, что и оригинальный, но с другой формой фото
i = 0
new_x_train = np.zeros((30_000, img_width, img_height, 3))
for img in x_train:
  res_img = resize_dataSet(img)
  new_x_train[i] = res_img
  i = i + 1 
print(f'New dataset shape is {new_x_train.shape}')

# после создания копии датасета - старый можно удалить
del(x_train)

"""Выделение проверочной выборки"""

new_x_train, x_val, y_train, y_val = train_test_split(new_x_train, y_train, test_size=0.3, random_state=0)
print(f'new_x_train shape is {new_x_train.shape}')
print(f'y_train shape is {y_train.shape}')
print(f'x_test shape is {x_test.shape}')
print(f'y_test shape is {y_test.shape}')
print(f'x_val shape is {x_val.shape}')
print(f'y_val shape is {y_val.shape}')

"""Создание модели"""

base_model = Xception(input_shape=(img_width, img_height, 3), include_top=False)

#выходные значения
x = base_model.output

#функц стиль добавления слоев 
x = GlobalAveragePooling2D()(x)
predictions = Dense(nb_classes, activation='softmax')(x)

#создание сети 
model = Model(base_model.input, predictions)
model.summary()
model.compile(optimizer = "rmsprop", loss = "categorical_crossentropy", metrics = ["accuracy"])

"""Создание генераторов"""

datagen = ImageDataGenerator(
featurewise_center=False,
samplewise_center=False,
featurewise_std_normalization=False,
samplewise_std_normalization=False,
rotation_range=transformation_ratio,
width_shift_range=transformation_ratio,
height_shift_range=transformation_ratio,
shear_range=transformation_ratio,
zoom_range=transformation_ratio,
horizontal_flip=True,
vertical_flip=False,
fill_mode='nearest',
rescale=1./255
)


train_generator = datagen
train_generator.fit(new_x_train)
train_generator = datagen.flow(new_x_train, y_train, batch_size=batch_size)

val_generator = ImageDataGenerator(rescale=1./255)
val_generator.fit(x_val)
val_generator = val_generator.flow(x_val, y_val, batch_size=batch_size)

test_generator = ImageDataGenerator(rescale=1./255)
test_generator.fit(x_test)
test_generator = test_generator.flow(x_test, y_test, batch_size=batch_size)

"""Возможность ранней остановки"""

callbacks_list = [
    EarlyStopping(monitor='accuracy', # наблюдение за точностью
                  patience=2,  #  если точность не повышается (2 эпохи), то тренировка прекращается 
                  verbose=0)
]

"""Тренировка"""

epochs = 30
history = model.fit(train_generator,
                    steps_per_epoch=train_generator.n // batch_size,
                    epochs = epochs,
                    validation_data=val_generator,
                    validation_steps=val_generator.n // batch_size,
                    verbose=1,
                    callbacks=callbacks_list).history

"""Сохранение и загрузка модели"""

model.save("/content/drive/MyDrive/lab2/model.h5")

model = load_model("/content/drive/MyDrive/lab2/model.h5")

"""Графики"""

# ГРАФИКИ
def smooth_curve(points, factor=0.8):
  smoothed_points = []
  for point in points:
    if smoothed_points:
      previous = smoothed_points[-1]
      smoothed_points.append(previous * factor + point * (1 - factor))
    else:
      smoothed_points.append(point)
  return smoothed_points

def draw_smooth_graph(history):
  loss_values = history["accuracy"]
  validation_loss_values = history["val_accuracy"]

  epochs = range(1, len(history['accuracy']) + 1)

  plt.plot(epochs, smooth_curve(loss_values), 'b', label='Training accuracy')
  plt.plot(epochs, smooth_curve(validation_loss_values), 'r', label='Validation accuracy')
  plt.title('Training and validation accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Value')
  plt.legend()
  plt.show()

draw_smooth_graph(history)

"""Создание шума"""

def normal_noise(image):
  percent = 0.2
  mean = 0.   # среднее значение пикселей
  std = 1.   # стандартное отклонение
  noisy_image = image + np.random.normal(mean, std, image.shape) * percent
  noisy_image_clipped = np.clip(noisy_image, 0, 255)
  return noisy_image_clipped

"""Наложение шума"""

#random image
img = new_x_train[random.randint(0, len(new_x_train)-1)]

# создание копии фотографии для вывода на экран
img_to_show = img.copy().astype('float32')
plt.imshow(img_to_show)
plt.show()

noised_img = normal_noise(img)

img_to_show = noised_img.copy()
plt.imshow(img_to_show)
plt.show()

"""Создание генератора с шумом"""

noisy_generator = ImageDataGenerator(rescale=1./255, preprocessing_function=normal_noise)
noisy_generator.fit(new_x_train)
noisy_generator = noisy_generator.flow(new_x_train, y_train, batch_size=batch_size)

"""Точность на зашумленном тестовом наборе"""

loss, accuracy = model.evaluate(noisy_generator, verbose = 1)
print(loss, accuracy)